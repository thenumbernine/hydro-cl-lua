//// MODULE_NAME: <?=FVSolver?>
//// MODULE_DEPENDS: <?=Solver?>
// I don't plan to use this name in the c++ code anywhere
// I just need a unique name for all the fvsolver kernels to reference

// used by all the finite volume solvers

namespace <?=Solver?> {

struct FVSolver {
<?
local useFluxLimiter = solver.fluxLimiter > 1
	and flux.usesFluxLimiter -- just flux/roe.lua right now
?>
	// this one gets overridden by WENO and EulerBurgers
	template<typename Flux>
	static inline void calcFlux(
		constant Solver const & solver,
		global Cons * const fluxBuf,
		global const <?=solver.getULRArg?>,
		realparam const dt,	//not used by HLL, just making this match Roe / other FV solvers
		global Cell const * const cellBuf
	) {
		<?=SETBOUNDS?>(solver.numGhost, solver.numGhost-1);

		int const indexR = index;
		auto const & cellR = cellBuf[index];

		<? for side=0,solver.dim-1 do ?>{
			constexpr int const side = <?=side?>;

			real const dx = solver.grid_dx[side];

			int const indexL = index - solver.stepsize[side];
			auto const & cellL = cellBuf[indexL];

			real3 xInt = cellR.pos;
			xInt[side] -= .5 * dx;

			int const indexInt = side + dim * index;
			auto & flux = fluxBuf[indexInt];


<? if solver.coord.vectorComponent == 'cartesian'
	or solver.coord.vectorComponent == 'anholonomic'
then ?>
//// MODULE_DEPENDS: <?=cell_areas?>
			real area = cell_areas<side>(solver, xInt);
<? else ?>
			real area = 1.;
			for (int i = 0; i < dim; ++i) {
				if (i != side) {
					area *= solver.grid_dx[i];
				}
			}
<? end ?>
			if (area <= 1e-7) {
				for (int j = 0; j < numStates; ++j) {
					flux[j] = 0;
				}
			} else {

				<?=solver:getULRCode():gsub('\n', '\n\t\t\t')?>

				//the single act of removing the copy of the U's from global to local memory
				// increases the framerate from 78 to 127
//// MODULE_DEPENDS: <?=cons_parallelPropagate?>
				auto const ppUL = <?=cons_parallelPropagate?><?=side?>(UL, cellL.pos, .5 * dx);
				auto const ppUR = <?=cons_parallelPropagate?><?=side?>(UR, cellR.pos, -.5 * dx);

				auto const n = Normal::forSide<side>(xInt);

<?
if useFluxLimiter then
?>				//this is used for the flux limiter
				//should it be using the coordinate dx or the grid dx?
				//real dt_dx = dt / cell_dxs<side>(solver, xInt);
<?
	if solver.coord.vectorComponent == "cartesian"
	and not require "hydro.coord.cartesian":isa(solver.coord)
	then
?>
//// MODULE_DEPENDS: <?=cell_dxs?>
				real const dt_dx = dt / cell_dxs<side>(solver, xInt);
<? 	else
?>				real const dt_dx = dt / dx;
<? 	end
?>
				real3 xIntL = xInt; xIntL[side] -= dx;
				real3 xIntR = xInt; xIntR[side] += dx;

				int const indexR2 = indexR + solver.stepsize[side];
				int const indexL2 = indexL - solver.stepsize[side];
				<?=solver:getULRCode{indexL = "indexL2", indexR = "indexL", suffix="_L"}:gsub("\n", "\n\t\t\t")?>
				<?=solver:getULRCode{indexL = "indexR", indexR = "indexR2", suffix="_R"}:gsub("\n", "\n\t\t\t")?>

//// MODULE_DEPENDS: <?=cons_parallelPropagate?>
				auto const ppUL_L = <?=cons_parallelPropagate?><?=side?>(UL_L, xIntL, 1.5 * dx);		//xIntL2?
				auto const ppUL_R = <?=cons_parallelPropagate?><?=side?>(UL_R, xIntL, .5 * dx);
				auto const ppUR_L = <?=cons_parallelPropagate?><?=side?>(UR_L, xIntR, -.5 * dx);
				auto const ppUR_R = <?=cons_parallelPropagate?><?=side?>(UR_R, xIntR, -1.5 * dx);		//xIntR2?

				auto const & cellR2 = cellBuf[indexR2];
				auto const & cellL2 = cellBuf[indexL2];

<?
end
?>
//// MODULE_DEPENDS: calcFluxForInterface
				// since the flux args change depending on the flux (1st order vs 2nd order)
				// how about the flux itself does the stencil calcs?
				// then the function signatures would all match.
				// but the stencils need to be parallel-propagated into the current interface's frame
				// so I could provide a function to do that?
				flux = Flux::calcFluxForInterface(
					solver,
					
					ppUL,
					ppUR,
					cellL,
					cellR,
					xInt,
					
					n
<? if useFluxLimiter then ?>
					,
					dt_dx,
					ppUL_L,
					ppUL_R,
					cellL2,
					cellL,
					xIntL,
					ppUR_L,
					ppUR_R,
					cellR,
					cellR2,
					xIntR
<? end ?>
				);

				//while we're here how about other solvers that might want to modify the flux?  like adding the viscous flux to the update?
				//or should the viscous flux only be added separately?  in case its eigenvalues change the euler flux enough to overstep the CFL or something
<?
for _,op in ipairs(solver.ops) do
	if op.addCalcFluxCode then
?>				<?=op:addCalcFluxCode()?>
<? 	end
end
?>
			}
		}<? end ?>
	}

	static inline void calcDerivFromFlux(
		constant Solver const & solver,
		global Cons * const derivBuf,
		global Cons const * const fluxBuf,
		global Cell const * const cellBuf
	) {
		<?=SETBOUNDS_NOGHOST?>();
		auto & deriv = derivBuf[index];
		auto const & cell = cellBuf[index];
		real3 const x = cell.pos;

/*<?--[[
volume vs area ...
in Cartesian grids the volume is dx * dy * dz  = int dx dy dz
	volume = int |J| |dy/dx| dx0 dx1 dx2
	area_j = int |S| |dy/dx| dx-not-j

in curvilinear holonomic (coordinate)
	- basis is given by the int (coordinate volume form) dx dy dz
	-
in curvilinear anholonomic basis
	- basis is given by the int (coordinate volume form) dx dy dz
	- some flux rescaling for non-scalars is needed
in curvilinear coords, cartesian basis:
	- basis is given by the int (coordinate volume form) dx dy dz
	-

--]]?>*/
<? if solver.coord.vectorComponent == "holonomic"
or require "hydro.coord.cartesian":isa(solver.coord)
then ?>
		real volume = 1.;
		for (int i = 0; i < dim; ++i) {
			volume *= solver.grid_dx[i];
		}
<? else ?>
		real const volume = cell.volume;
<? end ?>

		<? for side=0,solver.dim-1 do ?>{
			constexpr int side = <?=side?>;
			int const indexIntL = side + dim * index;
			auto const & fluxL = fluxBuf[indexIntL];

			int const indexIntR = indexIntL + dim * solver.stepsize[side];
			auto const & fluxR = fluxBuf[indexIntR];

			real3 xIntL = x; xIntL[side] -= .5 * solver.grid_dx[side];
			real3 xIntR = x; xIntR[side] += .5 * solver.grid_dx[side];

			//This is the covariant finite volume code that that represents the gradient of the metric determinant
			//All other covariant terms should be accounted for in the equation source update
			//U^i_;t + F^ij_;j  = 0
			//U^i_,t + F^ij_,j + Gamma^j_kj F^ik + Gamma^i1_kj F^i1^k + ... + Gamma^in_kj F^in^k = 0
			//					(metric det gradient)
<? if solver.coord.vectorComponent == "holonomic"
or require "hydro.coord.cartesian":isa(solver.coord)
then ?>
			real areaL = 1.;
			for (int i = 0; i < dim; ++i) {
				if (i != side) {
					areaL *= solver.grid_dx[i];
				}
			}
			real areaR = areaL;
<? else ?>
//// MODULE_DEPENDS: <?=cell_area_i?>
			real areaL = cell_areas<side>(solver, xIntL);
			real areaR = cell_areas<side>(solver, xIntR);
<? end ?>

			/*
			for
			gridSize={512,512}
			mins={-1,1}
			maxs={1,1}
			grid_dx = {2/512,2/512}
			volume=1/65536
			invVolume=65536
			areaL = 256
			areaR = 256
			somewhere half is failing to do this math

			ok it turns out 65519 is the largest number stored in half precision
			any larger and it just maps to inf
			thats why 512x512 doesn't work, 512x511 doesn't work, but 512x510 does work
			the limit is the volume

			how to circumvent this?  for higher grid resolutions, maintain volume by increasing mins/maxs of the initcond domain.
			*/
			if (volume > 1e-7) {
				real const invVolume = 1. / volume;
				if (areaL <= 1e-7) areaL = 0.;
				if (areaR <= 1e-7) areaR = 0.;
				areaL *= invVolume;
				areaR *= invVolume;
<? -- TODO get rid of this, it's only used by the maxwell and glm-maxwell eqns
if eqn.postComputeFluxCode then ?>
				Cons flux = {.s={0}};
				for (int j = 0; j < numIntStates; ++j) {
					flux[j] =
						fluxR[j] * areaR
						- fluxL[j] * areaL;
				}
<?=eqn:postComputeFluxCode()?>
				for (int j = 0; j < numIntStates; ++j) {
					deriv[j] -= flux[j];
				}

<? else -- not postComputeFluxCode ?>
				for (int j = 0; j < numIntStates; ++j) {
					deriv[j] -=
						fluxR[j] * areaR
						- fluxL[j] * areaL;
				}
<? end -- postComputeFluxCode ?>
			}
		}<? end ?>
	}
};

}	// namespace <?=Solver?>

//// MODULE_NAME: <?=calcDerivFromFlux?>
//// MODULE_DEPENDS: <?=FVSolver?>

kernel void <?=calcDerivFromFlux?>(
	constant <?=solver_t?> const * const psolver,
	global <?=cons_t?> * const derivBuf,
	global <?=cons_t?> const * const fluxBuf,
	global <?=cell_t?> const * const cellBuf
) {
	auto const & solver = *psolver;
	<?=Solver?>::FVSolver::calcDerivFromFlux(solver, derivBuf, fluxBuf, cellBuf);
}
